# -*- coding: utf-8 -*-
"""DAY-7.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1S8TwmUBKeKuSy8qRS7ztZi-yBnej-uCi
"""

import numpy as np        # For numerical operations and arrays
import pandas as pd         # For data loading, manipulation, and analysis
import matplotlib.pyplot as  plt # for data visualzing
import seaborn as sns        # For advanced data visualization with better styling

data=pd.read_csv("/content/Cancer_Data.csv") # Import dataset
data # display the data

data.info() # To describing the dataset

data.describe()

# Convert diagnosis column to binary
if 'diagnosis' in data.columns:
    data['diagnosis'] = data['diagnosis'].map({'M': 1, 'B': 0})
else:
    raise ValueError("The dataset must contain a 'diagnosis' column with 'M' and 'B' values.")

x=data[["radius_mean","texture_mean"]] # Assigning the input attributes to the x
x.head() # Displaying the top 5 x Values

y = data["diagnosis"] # Assigning the output attribute to the y
y.head() # Displaying the top 5 y values

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_scaled = scaler.fit_transform(x)

from sklearn.model_selection import train_test_split # Spliting the data into training and testing datasets
x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=42)

x_train # Displaying the x_train

y_train # Displaying the y_train

from sklearn.svm import SVC # import SVC(Support Vector Classification) from SVM

model=SVC(kernel="linear") # Here we are using kernel=linear to trnaform input data and easier to seperate the hyperplane

model.fit(x_train,y_train) # Training the model by giving x_train and y_train

rbf=SVC(kernel='rbf',gamma='scale')  # Using RBF kernel for non-linear decision boundaries

rbf.fit(x_train,y_train) # Training the model by giving x_train and y_train

model.predict(x_test) # Testing the model by giving x_test

rbf.predict(x_test) # Testing the model by giving x_test

y_test #displaying the y_test

model.support_vectors_ # which is used to get support vectors

model.n_support_ # gets the no of supporting vectors for each class

model.predict([[1.56,4.89]]) # Testing the model by giving new input attributes

from sklearn.metrics import mean_squared_error,r2_score,classification_report # importing evaluation metrics to see how well model is working

mse=mean_squared_error(y_test,model.predict(x_test)) # calculating the mean_squared_error
mse # displaying the mse

r2=r2_score(y_test,model.predict(x_test)) # Calculating the r2_score
r2 # displaying the r2_score

cr=classification_report(y_test,model.predict(x_test)) # Calculating the classification report
cr # displaying the classification report

cr2=classification_report(y_test,rbf.predict(x_test)) # Calculating the classification report
cr2 # displaying the classification report

# Decision boundary plot function
# Function to visualize the decision boundary of an SVM model
def plot_svm_boundary(model, X, y, title):
    h = 0.02  # Step size in the meshgrid
    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1  # Range for x-axis
    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1  # Range for y-axis
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                         np.arange(y_min, y_max, h))  # Create a meshgrid
    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])  # Predict the class for each point in the meshgrid
    Z = Z.reshape(xx.shape)  # Reshape the predictions to match the meshgrid shape

    plt.contourf(xx, yy, Z, alpha=0.8, cmap=plt.cm.coolwarm)  # Plot decision boundary
    plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k', cmap=plt.cm.coolwarm)  # Plot the data points
    plt.title(title)  # Title for the plot
    plt.xlabel('radius_mean')  # X-axis label
    plt.ylabel('texture_mean')  # Y-axis label
    plt.show()  # Display the plot

# Visualize decision boundaries
plot_svm_boundary(model,X_scaled,y,"SVM with Linear Kernel")
plot_svm_boundary(rbf,X_scaled,y,"SVM with RBF Kernel")

# Hyperparameter tuning using GridSearchCV
from sklearn.model_selection import GridSearchCV

# Hyperparameter tuning using GridSearchCV
params = {'C': [0.1, 1, 10], 'gamma': ['scale', 0.01, 0.001]}
grid = GridSearchCV(SVC(kernel='rbf'), params, cv=5)
grid.fit(x_train, y_train)

print(grid.best_params_)